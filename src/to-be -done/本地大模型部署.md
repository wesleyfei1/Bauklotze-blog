






输入了start-ollama.bat之后会自动在本地拉起来一个ollama的server, 如果没有的话，那么输入ollama run llama3.1:8b会通过开启ollama那个程序作为server

| **命令**                   | **用途**       | **备注**                      |
| ------------------------ | ------------ | --------------------------- |
| **`ollama serve`**       | 启动 Ollama 服务 | 所有的命令运行前，后台服务必须是启动状态。       |
| **`ollama run <name>`**  | 运行并进入对话界面    | 如果本地没有该模型，会自动执行 `pull`。     |
| **`ollama list`**        | 列出本地已下载模型    | 可以看到模型的 ID、大小和更新时间。         |
| **`ollama pull <name>`** | 拉取/更新模型      | 仅下载不运行，适合提前准备实验环境。          |
| **`ollama rm <name>`**   | 删除指定模型       | 及时清理不常用的模型以释放磁盘。            |
| **`ollama ps`**          | 查看当前正在运行的模型  | **核心命令：** 可以看到模型占用了多少显存/内存。 |
```powershell
taskkill /f /im ollama*#先终止所有别的东西
#当你将环境变量配置好之后
start-ollama.bat# 现在好像已经不需要了
ollama run qwen2.5:7b#如果没有下载的话，这会自动下载这个模型
ollama pull bge-m3
```
**本地模型的api调用**
```python
from openai import OpenAI
client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama') 
# 开启流式输出

response = client.chat.completions.create(

    model="qwen2.5:7b",

    messages=[{"role": "user", "content": "请分析实验数据。"}],

    stream=True,  # 关键点：开启流式传输

    extra_body={"options": {"num_ctx": 4096}}

)
# 实时迭代获取每一个 token

for chunk in response:

    if chunk.choices[0].delta.content is not None:

        print(chunk.choices[0].delta.content, end="", flush=True)
```
https://gemini.google.com/share/8b48839519a4

以及视频
https://www.youtube.com/watch?v=XFKRfhxlELg


## 2.6 大模型部署序
```
from openai import OpenAI

client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama', # 随便填，Ollama 不需要 key
)

response = client.chat.completions.create(
  model="qwen2.5-coder:7b",
  messages=[{"role": "user", "content": "帮我写一个生成推荐系统负采样样本的函数"}]
)
print(response.choices[0].message.content) 从而可以调用
```

|**组件**|**在 Web 世界中相当于**|**在 Ollama 中相当于**|
|---|---|---|
|**原始权重**|源代码|Hugging Face 上的 `.safetensors` 文件|
|**推理加速**|编译器 (gcc/clang)|**`llama.cpp`** (负责把模型压榨出最高性能)|
|**容器化/管理**|**Docker**|**Ollama** (负责打包、版本控制、API 暴露)|
|**交互界面**|浏览器 / App|`ollama run` 命令行或 Open WebUI 等前端|
